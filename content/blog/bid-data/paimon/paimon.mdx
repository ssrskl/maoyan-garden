---
title: Paimon 数据湖
description: Apache Paimon 是一个专门为CDC处理、流计算而生的实时数据湖存储，支持高速数据摄取、变化数据跟踪和高效实时分析。
date: 2025-09-02 19:18:00
tags: ["Paimon", "数据湖","流式计算","流批一体"]
published: true
status: growing
---

# Paimon

## Paimon介绍

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20260123210341-g9652wi.png)

‍

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20260123210912-bkccfzy.png)

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20260123210925-hd4tyjx.png)

## 为什么选择Paimon

**准实时替换离线**：一条 SQL 把 MySQL 全库搬到 Paimon，Schema 变更自动同步

- 分钟级可见：Checkpoint 1 min 可见
- 百倍存储节省：LSM 复用文件，10 TB 表每日 1 GB 更新，历史快照仅增量占用
- 离线视图秒级创建：`CREATE TAG 20251111` 立即产生快照，供离线跑批，同时对于一些准实时 5min/30min/1h 指标都可以快速支持。

**实现湖仓一次存储**：存储成本居高不下，很多实时性要求不高的场景，其实没有必要用成本较高的分布式存储服务来支持。Paimon 解决离线、实时双跑双存储问题，实现离线实时同时读一张表，降低存储成本。当前很多实时 realtime 表存到离线一份导致存储压力过大。

**Kappa 的实时数仓，缺陷如下**：

- Kafka 无法支持海量数据存储。对于海量数据量的业务线来说，Kafka 一般只能存储非常短时间的数据，比如最近一周，甚至最近一天。
- Kafka 无法支持高效的 OLAP 查询。大多数业务都希望能在 DW/DDWS 层支持即席查询，但是 Kafka 无法非常友好地支持这样的需求，无法复用目前非常成熟的基于离线数仓的数据血缘、数据质量管理体系。需要重新实现一套数据血缘、数据质量管理体系。
- Kafka 不支持 update/upsert，目前 Kafka 仅支持 append。

**实时数据写入 hive 表会导致大量小文件产生**：  
之前我们在讲小文件那节课也提到了实时数据写入离线表不会合并，如果要合并需要起调度任务，但这样效率太慢了（业务方在查询时仍然会收到小文件干扰），总不能 5min 跑一次离线调度去刷吧。。。

# 元数据层

paimon 使用的也是类似于 Hive 的双层架构，分为元数据层和数据层，但是其元数据层相对于 Hive 只用于指定 HDFS 数据路径在哪里来说要更复杂一些。

每次写操作都会写出一组文件，包含：

- 一个快照根文件（snapshotfile)
- 两个清单列表文件（manifest-list））——**base与delta**
- 一个索引清单文件（indexmanifest）—暂时可以不关注
- 若干清单文件（manifestfiles)
- 若干数据文件

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20251229154649-cj3sq8g.png)

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20260219235154-xp58775.png)

左边是逻辑结构，右边是物理结构
<SplitLayout layout="1/2">
![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20251229153650-d42v8mx.png)

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20251229153444-ru2tzwe.png)
</SplitLayout>



## Snapshot快照

快照文件存放在 <kbd>​`snapshot`​</kbd>​ 目录下，文件名以表的版本号为后缀。同时会维护 <kbd>​`LATEST`​</kbd>​ 和 <kbd>​`EARLIEST`​</kbd> 两个软链接，分别指向最新和最早的快照文件。

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20251229153937-r6ynqit.png)

可以看到，在快照文件包含的信息中，最重要的就是`baseManifestList`​ ，`DeltaManifestList`​以及`Manifest`文件。

1. baseManifestList：表示的是存量的Manifest列表，即这个快照之前的Manifest文件，生成方式就是上一个快照的存量（baseManifestList）+增量（DeltaManifestList），就类似于我们想要创建一个今日全量表，可以用昨日全量表+昨日增量表来计算。
2. DeltaManifestList：表示的是这个快照中新增的Manifest文件
3. 使用 Base 和Delta 双ManifestList的好处：

   1. 流式读取：**只需要扫描DeltaManifestList即可获取本次新增的文件**
   2. 批式查询：**合并Base 和 Delta 即可获得完整的视图**

## Schema

Schema 文件的版本从 0 开始递增。目前 Paimon 会保留所有历史 Schema 版本。原因是多个快照可能依赖同一个 Schema 文件（例如表结构长时间未发生变更），如果贸然删除旧版本 Schema，可能导致依赖它的历史快照无法正确读取。Schema 文件采用 JSON 格式，主要包含以下字段：

- fields：数据字段列表，描述表包含的所有列及其类型。
- partitionKeys：分区字段名称列表，定义表的分区键（不可修改）。
- primaryKeys：主键字段名称列表，定义表的主键（不可修改）。
- options：Map\<String, String\> 类型，用于存储表的各种配置参数（无序）。

‍

# 数据层

表的数据文件按两层进行划分，其实也是常见的分区分桶类型。

**分区：** 按分区列切分（常见为时间维度），便于查询剪枝，读取的时候可以整目录跳过，提升效率

**桶：** 每个分区再细分为若干桶，桶数可固定或动态。提供并行读写，**每一个桶对应了一个 LSM-Tree**。

- 固定桶：按照主键进行哈希映射
- 动态桶：在全局桶索引中维护一个 **“键->桶”** 的映射关系

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20251229161841-9ud4g7a.png)

## LSM—Tree

### SSTable

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20251229162320-ghtvfbr.png)

在 LSM-Tree 中，最核心的数据结构就是SSTable，全称是**SortedStringTable**，SSTable是一种**不可变、按key有序、可持久化的键值结构**，其key和value均可为任意字节序列，它既支持按指定key的精确检索，也支持按key区间进行选代扫描。

SSTable 的读取：当读取 SSTable 中的内容的时候，首先读取索引信息，通过**二分查找**定位到key 的偏移量，再讲数据读回。

### 写入流程

<SplitLayout layout="1/2">
![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20251229162357-aon0cxi.png)

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20260219235238-vhctueu.png)
</SplitLayout>

<SplitLayout layout="1/2">
![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20260219235335-oypmlq9.png)

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20260219235346-3xo9b3s.png)
</SplitLayout>

LSM-Tree的写路径可以概括为五步：

1. 写请求首先追加到WAL Log，用作故障恢复。
2. 当写完WAL Log后，会把该条数据写入内存的SSTable里面（删除是墓碑标记，更新是新记录一条数据），也称Memtable。注意：为了维持有序性，在内存中一般采用红黑树或者跳跃表相关的数据结构。
3. 当Memtable超过一定的大小后，会在内存里面冻结，变成不可变的Memtable，同时为了不阻塞写操作，需要新生成一个Memtable继续提供服务。
4. 把内存里面**不可变的Memtable dump到硬盘上的SSTable层中**，此步骤称为**flush**（也称为  
   **Minor Compaction**）。
5. 当每层的磁盘上的SSTable的体积超过一定的大小或者个数，也会周期地进行异步合并。

   1. 此步骤也称为MajorCompaction，这个阶段会真正地清除掉被标记删除的数据以及多版本数据的合并，避免浪费空间。
   2. 注意：由于SSTable都是有序的，我们可以直接采用mergesort进行高效合并。

### 读取流程

读取则分为了两步：

1. 现在内存中查找，命中了就返回。
2. 若内存中没有命中，则从上到下扫描磁盘中的SSTable。

每一个层级都有很多的 SSTable，所以为了快速的判断这个 SSTable 中是否有想要的数据，使用**布隆过滤器（Bloom Filter）** 来做优化。如果布隆过滤器判断不在则跳过这个SSTable。

### 布隆过滤器

布隆过滤器（Bloom Filter）是一种空间高效的概率型数据结构，主要用于**快速判断一个元素是否可能存在于一个大规模集合中**。

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20251229163623-4hkbbel.png)

非常的节省空间，但是有时候可能会误判，**将不在这个集合中的误判为在这个集合中**的。

## Paimon 和 HBase的 LSM 对比

Paimon和HBase都使用LSM树，但需要记住三点区别：

1. Paimon依赖Flink，可以不使用WAL；HBase使用WAL。
2. Paimon通过mergeengine实现了数据的**合并逻辑**，例如聚合累加、first_row等；HBase不支持。
3. compaction行为：Paimon是利用Flink来做。HBase是使用HBaseServer来做。

# 数据一致性

## 两阶段提交

**预提交（Pre-Commit）** ：参与者完成本地事务，并持久化“可回滚”的数据

**提交（Commit）** ：协调者确认所有的参与者完成预提交后统一发出 Commit 指令

在分布式系统中，可以使用两阶段提交来实现事务性从而保证数据的一致性，两阶段提交分为：**预提交阶段与提交阶段**，通常包含两个角色：**协调者与执行者**，协调者用于管理所有执行者的操作，执行者用于执行具体的提交操作，具体的操作流程：  
1.首先协调者会送预提交(pre-commit)命令有的执行者  
2.执行者执行预提交操作然后发送一条反馈(ack)消息给协调者  
3.待协调者收到所有执行者的成功反馈，则发送一条提交信息（commit)给执行者  
4.执行者执行提交操作

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20251229191232-nlka2pi.png)

## Flink的两阶段提交

<SplitLayout layout="1/2">
![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20251229191434-spu7sd9.png)

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20251229191308-pf3fl33.png)
</SplitLayout>

Flink 的两阶段提交是为了保证端到端的精确一次性，主要依靠Checkpoint 的机制来实现，我们可以看一下 Checkpoint 的实现流程：

1. JobManager 周期性的发送 Checkpoint命令
2. 当**source端**收到执行指令后会**产生一条barrier消息插入到input消息队列中**，当处理到barrier  
   时**会执行本地checkpoint**,并且会**将barrier发送到下一个节点**，当checkpoint完成之后会发送  
   条ack信息给jobMaster;
3. 当DAG图中所有节点都完成checkpoint之后，jobMaster会收到来自所有节点的ack信息，那么就表示一次完整的checkpoint的完成；
4. JobMaster会**给所有节点发送一条callback信息**，表示通知checkpoint完成消息，这个过程是异步的，并非必须的，方便做一些其他的事情，例如kafkaoffset提交到kafka。

## Paimon的两阶段提交

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20251229191622-4dfij08.png)

Paimon **将Pre-commit 阶段放在了数据文件层，将Commit 阶段放在了元数据层**，实现了端到端的一致性。

- 阶段一：Flink Writer Task 将数据追加到存储，不修改任何的元数据，产出的数据包括压缩的文件数据以及删除向量等，当阶段失败的时候，直接将文件丢弃即可。
- 阶段二：Flink Committer Operator 检测snapshot 的 latest 是否为base，如果失败的话则直接丢弃临时的快照和清单文件即可。

# 主键表

**主键表（Primary Key Table）** 是默认的表类型，也是最核心的表模式之一。它支持**实时插入、更新和删除记录**，非常适合处理 CDC（Change Data Capture）变更数据或流式 upsert 场景。

## 分桶策略

分桶是Paimon 表的最小读写单元，毕竟一个桶中就是一个LSM-Tree ，主键表的分桶策略分为了**固定分桶**和**动态分桶**。

**固定分桶**，即直接定义`bucket` 的数量为一个固定值


```sql
CREATE TABLE MyTable (
    id BIGINT,
    name STRING,
    dt STRING,
    PRIMARY KEY (id, dt) NOT ENFORCED
) PARTITIONED BY (dt) WITH (
    'bucket' = '4',                  -- 固定桶数
    'merge-engine' = 'deduplicate'   -- 合并引擎
);
```

动态分桶，`bucket`​的参数值为<kbd>-1</kbd>，动态分桶的主键可以包含也可以不包含分区字段。

## 分桶原理

当一条新的数据到来的时候，需要判断两个问题：

1. 这个数据是新的吗？
2. 如果不是新的，那么原来的数据在哪里？

对于**固定分桶**，由于主键相同，所以新数据计算的哈希值也相同，可以直接找到原来的数据所在的分区和 bucket。

但是在跨分区的更新场景中，数据的主键是不变的，但是分区可能是会变化的，例如使用地区作为分区，那么当用户的地区发生改变的时候，新数据和老数据则不在同一个分区中，所以系统需要具备全局的视角，能够根据主键快速的定位到原先的数据所在的分区和分桶。**GlobalIndexAssigner就是用于维护一个全局的主键对应分区和分桶的映射关系。**

## 桶策略优化

‍

# Append表

创建 Paimon 表的时候如果**没有定义主键**，那么该表即为 **Append Only** 表，这种表类似于高级版的 Hive 表，只接受数据的插入操作，适用于**不需要流式更新的场景**，例如**日志数据，事件埋点**等。Append Only 表的**Compaction操作仅用于合并小文件**，不涉及数据的合并和去重。

Append Only 表有两种模式，<kbd>​`Scalable`​</kbd>​ 表和 <kbd>​`Queue`​</kbd> 表，主要区别在于 bucket 的设置、数据写入/读取的并行度、顺序保证以及适用场景。

## Scalable表

无固定 bucket，写入时动态分配或单 bucket 模式，即配置`'bucket' = '-1'`，适合高吞吐追加，在流读的场景下，无全局顺序保证（普通 append 模式），适用于高吞吐日志同步、批处理/OLAP 查询、需要高写入扩展性（如迁移 Hive 表）。

## Queue表

'bucket' \> 0（固定 bucket 数），可选 `'bucket-key' `​指定分桶键，数据按 bucket-key（类似 Kafka partition key）分桶，每个 bucket 独立。并行度受 bucket 数限制，但可通过增加 bucket 提升，**每个 bucket 内严格顺序**，流式读取时按写入顺序传输记录（exactly-once order per bucket），类似于多 Partition 的Kafka，可以作为低成本的消息队列，适用于需要顺序消费的流式场景（如事件流、CDC 日志），也可以模拟消息队列。

# Changelog Producer

Changelog（变更日志）是记录数据表中每一行数据如何随时间变化的日志流。它不仅记录"当前状态"，更记录“状态是如何变化的"。

**changelog** 是 Paimon 表中记录数据变更的增量数据流，包含完整的变更类型（RowKind），如：

- +I：插入（Insert）
- -U/+U：先删除旧值，再插入新值
- -D：删除（Delete）

Changelog文件**会进行单独的存储，存储在桶中**，生成的时机就是在Commit 的时候，快照生成的时候。

Paimon是否生成Changelog、如何生成，由表属性`changelog-producer`控制。该配置决定了  
Changelog的生成时机、准确性与性能开销。

## None

```properties
'changelog-producer' = 'none' --默认值
```

默认的情况下，就是None模式，完全不生成Changelog，仅维护表的 SSTable 文件，不生成 Changelog文件，无法支持真正的CDC 流读。

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20251229202909-2525kcr.png)

Flink 提供了`ChangelogNormalize`​ 算子，用于在消费端反推 Changelog，但是需要维护全局的状态，**资源消耗极大，不建议使用**。

## Input

```properties
'changelog-producer' = 'input'
```

信任上游的输入，直接将上游的Changelog进行持久化到 Changelog 文件中，但是上游的 CDC 流必须是标准且完整的CDC 流，若上游不完整，则会导致下游的逻辑错误。

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20251229203312-3nozzsj.png)

## Lookup

```properties
'changelog-producer' = 'lookup'
```

**在写入 Changelog 的时候，先查询一下当前表中是否存在相同主键的记录**。如果有的话则生成`-U/+U`，例如如下案例：

1. 更新一条数据（`id=1，val = 'Hello World'`）
2. 查询主键为 1 的数据，例如当前为（`id=1，val = 'Hello'`）
3. 生成`-U（id=1，val = 'Hello'）`​和`+U（id=1，val = 'Hello World'）`

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20251229203511-ya4f03h.png)


## Full-Compaction

```properties
'changelog-producer' = 'full-compaction'
'full-compaction.delta-commits' = '10'
```

写入时不生成 Changelog，而是在后台执行 Full Compaction 的时候，通过对比前后的完整数据集来统一生成Changelog。

触发机制由`full-compaction.delta-commits`控制，语义是多少次 Commit 才进行 Full Compaction，默认是每 Commit 一次，就触发一次。

![image](https://maoyanimagehost.oss-cn-guangzhou.aliyuncs.com/blog/paimon/image-20251229204603-9f0pn4w.png)

适用于离线批处理后仍然需要流式消费的混合架构，由于需要经过多次的 Commit 之后才会进行Full Compaction，生成 Changelog，所以 Changelog 的生成延迟较高。

# 快照管理与系统表

## 快照机制概述

**Snapshot（快照）**  是 Paimon 核心机制中的灵魂，它记录了表在某一特定时刻的完整状态。简单来说，每当有一批数据写入并提交（Commit）时，Paimon 就会生成一个新的 Snapshot。Snapshot 的功能主要有以下几个：

1. **实现数据读取的隔离性与一致性**

   Snapshot 保证了“读写分离”。当分析师在查询数据时，后台的流式写入任务可能正在进行。由于查询是基于某个特定的 Snapshot 进行的，读取操作不会受到正在写入的数据影响，从而保证了**事务的一致性**。
2. **支持增量消耗（Incremental Consumption）**

   这是流式数据湖的关键特性。流计算引擎（如 Flink）可以通过对比不同 Snapshot 之间的差异，准确地识别出哪些数据是新增的、哪些是修改的。
3. **版本回溯与时间旅行（Time Travel）**

   Snapshot 记录了历史状态。如果你发现最新的数据出错了，你可以通过指定 Snapshot ID 或时间戳，将查询切换到之前的状态。
4. **故障恢复（State Recovery）**

   在流处理中，如果作业崩溃，Flink 可以根据 Snapshot 信息快速定位到上一次成功提交的位置，确保数据**不丢不重（Exactly-once）** 。

## 快照保留策略

可通过表选项（**TableOptions**）或**SQL Hints**动态控制快照生命周期：

- ​`snapshot.num-retained.min`：最少保留快照数，防止意外删除2
- ​`snapshot.num-retained.max`：最多保留快照数，控制存储成本按业务需求 (如5~50)

SQL Hints 的用法：

```sql
INSERT INTO paimon.<database>.<table>
/*+ OPTIONS('snapshot.num-retained.min' = '2', 'snapshot.num-retained.max' = '10' */）
SELECT user_id, age, name, event_time, dt
FROM <data_source>
```


## 分区过期

[Manage Partitions #](https://paimon.apache.org/docs/master/maintenance/manage-partitions/)

针对于分区表，我们可以设置分区过期自动清理旧分区，即配置 TTL。


**如何确定一个分区是否过期：** 在创建分区表时，可以设置 `partition.expiration-strategy`​，该策略决定了如何提取分区时间，并将其与当前时间进行比较，以查看存活时间是否超过了`partition.expiration-time`。支持的过期策略值有：

- ​`values-time `​: 该策略比较**分区值中提取的时间与当前时间**，此策略为**默认策略**。
- ​`update-time `​: 该策略比较**分区的最后更新时间与当前时间**。 此策略适用于什么场景：

  - 分区值不是日期格式。
  - 只想保留最近 n 天/月/年更新的数据。
  - 数据初始化导入大量历史数据。

​`values-time` 策略案例：

```sql
CREATE TABLE t (...) PARTITIONED BY (dt) WITH (
    'partition.expiration-time' = '7 d',
    'partition.expiration-check-interval' = '1 d',
    'partition.timestamp-formatter' = 'yyyyMMdd'   -- 时间格式，values-time策略必须配置
);
insert into t values('pk', '2024-07-01');

CREATE TABLE t (...) PARTITIONED BY (other_key, dt) WITH (
    'partition.expiration-time' = '7 d',
    'partition.expiration-check-interval' = '1 d',
    'partition.timestamp-formatter' = 'yyyyMMdd',
    'partition.timestamp-pattern' = '$dt' -- 如果是多分区的话，指定时间分区字段是哪一个
);
```

​`update-time` 策略案例：

```sql
CREATE TABLE t (...) PARTITIONED BY (dt) WITH (
    'partition.expiration-time' = '7 d',
    'partition.expiration-check-interval' = '1 d',
    'partition.expiration-strategy' = 'update-time'
);

-- 现在更新了，所以将不会过期
insert into t values('pk', '2024-01-01');
-- 也支持非时间类型的分区
insert into t values('pk', 'par-1');
```

## 系统表

[System Tables](https://paimon.apache.org/docs/master/concepts/system-tables/)

Paimon提供了一套非常丰富的系统表，帮助用户更好地分析和查询 Paimon 表的状态，主要包含两种类型：

1. **数据系统表**：查询数据表的状态
2. **全局系统表**：查询整个目录的全局状态

数据系统表主要包含以下几种

![image](./imgs/image-20251230222656-86lv04o.png)

**监控快照的增长趋势：**

```sql
SELECT snapshot_id,commit_time,total_record_count
FROM paimon.prod.fact_sales$snapshots
ORDER BY commit_time DESC
LIMIT 10;
```

**给快照打 Tag，防止被清理：**

```sql
--为当前最新快照打标签"monthly_backup
CALL paimon.system.create_tag('prod.fact_sales','monthly_backup');
-- 后续可通过Tag查询：...$files/*+ OPTIONS('scan.tag-name′='monthly_backup')
```

# Tag 管理与Bucket 缩放

## Tag 管理

Paimon 自动维护的snapshot会按照过期策略被清理掉，那么就无法查询过期的快照信息。Tag 是针对于某个 Snapshot 的锚点，该 Tag 所对应的快照信息将会被完全的保留，包括数据信息。

# Hive 数据迁移到 Paimon

‍
